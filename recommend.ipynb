{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "#coding:utf-8\n",
     "import torch\n",
     "import torch.nn as nn\n",
     "from torch.autograd import Variable\n",
     "from torch.nn import utils as nn_utils\n",
     "import torch.optim as optim\n",
     "\n",
     "import numpy as np\n",
     "import heapq\n",
     "import time\n",
     "import random\n",
     "import tqdm\n",
     "import math\n",
     "import pandas as pd\n",
     "\n",
     "\n",
     "#use_cuda=1\n",
     "use_cuda=0 #还没测试过\n",
     "\n",
     "#############################设置随机种子，保证模型的可复现性\n",
     "np.random.seed(2020)\n",
     "torch.manual_seed(2020)\n",
     "random.seed(2020)\n",
     "\n",
     "if torch.cuda.is_available():\n",
     "    if use_cuda:\n",
     "        torch.cuda.manual_seed_all(2020)\n",
     "    else:\n",
     "        print('WARNING: You have a CUDA device, so you should probably run with --cuda')\n",
     "############################################################################################\n",
     "\n",
     "\n",
     "# Parameters\n",
     "# ==================================================\n",
     "if(use_cuda==1):\n",
     "    ftype = torch.cuda.FloatTensor\n",
     "    ltype = torch.cuda.LongTensor\n",
     "else:\n",
     "    ftype=torch.FloatTensor\n",
     "    ltype=torch.LongTensor\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "class Short_Term_GRU(nn.Module):\n",
     "    def __init__(self,user_sum,item_sum,batch_size,hidden_size,item_emb,user_emb,week_emb,check_time_emb):\n",
     "        super(Short_Term_GRU,self).__init__()\n",
     "        self.n_items=item_sum\n",
     "        self.n_user=user_sum\n",
     "        self.hidden_size=hidden_size\n",
     "        self.n_layers=1\n",
     "        self.batch_size=batch_size\n",
     "        self.item_emb=item_emb\n",
     "        self.user_emb = user_emb\n",
     "        self.week_emb=week_emb\n",
     "        self.check_time_emb=check_time_emb\n",
     "\n",
     "        self.rnn = nn.GRU(self.hidden_size +33, self.hidden_size, self.n_layers, batch_first=True)\n",
     "\n",
     "\n",
     "    def forward(self,item_input,week_input,time_input,seq_lengths_input,current_batch):\n",
     "        # item=Variable(torch.LongTensor([[1,2,3],[4,5,0]]))\n",
     "        # week=Variable(torch.LongTensor([[0,1,1],[7,7,7]]))\n",
     "        # time=Variable(torch.LongTensor([[24,0,0],[22,1,1]]))  #batch,sequence,hidden_size\n",
     "        seq_lengths = torch.LongTensor(seq_lengths_input).type(ltype)\n",
     "        #print(item_input,\"zheshi item_input\")\n",
     "        self.embed_item=self.item_emb(torch.LongTensor(item_input).type(ltype))\n",
     "        #print(self.embed_item,\"this is embedding item\")\n",
     "        self.embed_week=self.week_emb(torch.LongTensor(week_input).type(ltype))\n",
     "        self.embed_time=self.check_time_emb(torch.LongTensor(time_input).type(ltype))\n",
     "        input_item_relate=torch.cat([self.embed_item,self.embed_week,self.embed_time],dim=-1) #batch,sequence,3*hidden_size\n",
     "        # print(input_item_relate.shape)\n",
     "        # raise()\n",
     "        pack = nn_utils.rnn.pack_padded_sequence(input_item_relate, seq_lengths, batch_first=True)\n",
     "        # print(pack)\n",
     "        # raise()\n",
     "\n",
     "        # h0 = Variable(torch.zeros(self.n_layers, self.batch_size, self.hidden_size))\n",
     "        h0 = Variable(torch.zeros(self.n_layers, current_batch, self.hidden_size)).type(ftype)\n",
     "        out, _ = self.rnn(pack, h0)\n",
     "        # print(out)\n",
     "        unpacked=nn_utils.rnn.pad_packed_sequence(out,batch_first=True)\n",
     "        # print(\"短期\")\n",
     "        # print(unpacked)\n",
     "        # print(unpacked[0].shape)\n",
     "        # raise()\n",
     "        # time.sleep(10)\n",
     "        return unpacked, _\n",
     "\n",
     "        pass\n",
     "\n",
     "import time\n",
     "class Current_GRU(nn.Module):\n",
     "    def __init__(self,user_sum,item_sum,batch_size,hidden_size,item_emb,user_emb,week_emb,check_time_emb):\n",
     "        super(Current_GRU,self).__init__()\n",
     "        self.n_items=item_sum\n",
     "        self.n_user=user_sum\n",
     "        self.hidden_size=hidden_size\n",
     "        self.n_layers=1\n",
     "        self.batch_size=batch_size\n",
     "        self.item_emb=item_emb\n",
     "        self.user_emb=user_emb\n",
     "\n",
     "        self.week_emb=week_emb\n",
     "        self.check_time_emb=check_time_emb\n",
     "\n",
     "        self.rnn = nn.GRU(self.hidden_size +33, self.hidden_size, self.n_layers, batch_first=True)\n",
     "\n",
     "    def forward(self,item_input,week_input,time_input,seq_lengths_input,current_batch):\n",
     "        # item=Variable(torch.LongTensor([[1,2,3],[4,5,0]]))\n",
     "        # week=Variable(torch.LongTensor([[0,1,1],[7,7,7]]))\n",
     "        # time=Variable(torch.LongTensor([[24,0,0],[22,1,1]]))  #batch,sequence,hidden_size\n",
     "        # seq_lengths = [3, 2]\n",
     "\n",
     "        item_input = Variable(torch.LongTensor(item_input).type(ltype))\n",
     "        week_input = Variable(torch.LongTensor(week_input).type(ltype))\n",
     "        time_input = Variable(torch.LongTensor(time_input).type(ltype))  # batch,sequence,hidden_size\n",
     "        seq_lengths = [int(session_max_length) for _ in range(current_batch)]  #全局找到的\n",
     "\n",
     "\n",
     "        self.embed_item=self.item_emb(item_input)\n",
     "        # print(self.embed_item)\n",
     "        self.embed_week=self.week_emb(week_input)\n",
     "        self.embed_time=self.check_time_emb(time_input)\n",
     "        input_item_relate=torch.cat([self.embed_item,self.embed_week,self.embed_time],dim=-1) #batch,sequence,3*hidden_size\n",
     "        # print(input_item_relate.shape)\n",
     "        # raise()\n",
     "        pack = nn_utils.rnn.pack_padded_sequence(input_item_relate, seq_lengths, batch_first=True)\n",
     "        # print(pack)\n",
     "        # raise()  #输入，输出，隐藏尺寸\n",
     "        h0 = Variable(torch.zeros(self.n_layers, len(seq_lengths), self.hidden_size)).type(ftype)\n",
     "        out, _ = self.rnn(pack, h0)\n",
     "        #使用多余计算，然后利用花式索引找到对应的输出\n",
     "        # print(_)\n",
     "        # raise()\n",
     "        # print(out)\n",
     "        # raise()\n",
     "        unpacked=nn_utils.rnn.pad_packed_sequence(out,batch_first=True)\n",
     "        # print(unpacked)\n",
     "        out_index = unpacked[0][[_ for _ in range(current_batch)], seq_lengths_input-1].reshape(1, current_batch, self.hidden_size)\n",
     "\n",
     "        # time.sleep(10)\n",
     "        return out_index\n",
     "\n",
     "class Attention(nn.Module):\n",
     "    def __init__(self,user_sum,item_sum,batch_size,hidden_size):\n",
     "        super(Attention,self).__init__()\n",
     "        self.user_sum=user_sum\n",
     "        self.location_sum=item_sum\n",
     "        self.hidden_size=hidden_size\n",
     "        self.batch_size=batch_size\n",
     "        self.liner = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
     "        # self.liner_seqatta = torch.nn.Linear(self.hidden_size*2, self.hidden_size)\n",
     "        # self.context = torch.nn.Parameter(torch.FloatTensor(self.hidden_size))\n",
     "\n",
     "    # def forward(self,short_perference,current_perference,current_batch):\n",
     "    #     current_perference = current_perference.transpose(0,1)\n",
     "    #     # print(current_perference.shape)\n",
     "    #     # raise()\n",
     "    #     # print(current_perference.shape)\n",
     "    #     # raise()\n",
     "    #     current_perference = current_perference.repeat(1, short_perference[0].shape[1], 1)  #15表示最长长度\n",
     "    #     # print(current_perference.shape)\n",
     "    #     # print(current_perference)\n",
     "    #     # print(short_perference[0])\n",
     "    #     short_current_perference = torch.mul(short_perference[0], current_perference)\n",
     "    #     # print(short_current_perference)\n",
     "    #     short_current_line_input = short_current_perference.reshape(-1, self.hidden_size)  #-1表示送入神经网络的batch\n",
     "    #\n",
     "    #     # lineroutput = self.liner(short_current_line_input)  #加了个激活函数，效果好很多\n",
     "    #     lineroutput = torch.tanh(self.liner(short_current_line_input))\n",
     "    #     # print(lineroutput)\n",
     "    #     lineroutput1 = lineroutput.reshape(current_batch, -1, self.hidden_size)  #-1表示序列长度\n",
     "    #     # print(lineroutput)\n",
     "    #     lineroutput2 = torch.sum(lineroutput1, dim=-1)\n",
     "    #     softmax_lineroutput = torch.softmax(lineroutput2, dim=-1)\n",
     "    #     # print(softmax_lineroutput)\n",
     "    #     softmax_lineroutput = softmax_lineroutput.unsqueeze(-1)\n",
     "    #     # print(softmax_lineroutput)\n",
     "    #     softmax_lineroutput = softmax_lineroutput.repeat(1, 1, self.hidden_size)\n",
     "    #     # print(softmax_lineroutput)\n",
     "    #     # print(short_perference[0])\n",
     "    #     # print(softmax_lineroutput)\n",
     "    #     short_current_softmax_perference = torch.mul(short_perference[0], softmax_lineroutput)\n",
     "    #     # print(short_current_softmax_perference)\n",
     "    #     short_current_perference = torch.sum(short_current_softmax_perference, dim=1)\n",
     "    #     # print(short_current_perference)  # batch,hidden_size\n",
     "    #     return short_current_perference\n",
     "    #\n",
     "\n",
     "    def forward_batch_onebyone(self,short_perference,current_perference,current_batch,length_input):\n",
     "        current_perference=current_perference[0] #batch,hidden—size\n",
     "        lista=[]\n",
     "\n",
     "\n",
     "        #分层注意力(真）：\n",
     "        for i in range(current_batch):   #会比上面快那么一丁点，10秒左右吧\n",
     "            h_i=short_perference[0][i][:length_input[i]]\n",
     "            u_s=current_perference[i]\n",
     "            u_i=torch.tanh(self.liner(h_i))\n",
     "            us=torch.sum(torch.mul(u_i,u_s),dim=-1)\n",
     "            alpha_i=torch.softmax(us,dim=-1)  #长度*1\n",
     "            v=torch.sum(torch.mul(h_i,alpha_i.view(-1,1)),dim=0)\n",
     "            lista.append(v)\n",
     "        short_current_perference=torch.stack(lista)\n",
     "\n",
     "        return short_current_perference\n",
     "\n",
     "\n",
     "class Attention_perference(nn.Module):\n",
     "    def __init__(self,hidden_size):\n",
     "        super(Attention_perference,self).__init__()\n",
     "\n",
     "        self.hidden_size=hidden_size\n",
     "        self.liner = torch.nn.Linear(self.hidden_size, 1)\n",
     "        self.context=nn.Parameter(torch.FloatTensor(1,self.hidden_size))  #一行嵌入列的上下文向量\n",
     "        self.context.data.uniform_(-0.5,0.5)\n",
     "\n",
     "    def forward(self,a1,a2,a3,a4):\n",
     "        lista=[]\n",
     "        for one_a1,one_a2,one_a3,one_a4 in zip(a1,a2,a3,a4):\n",
     "            a_yuanshi=torch.stack([one_a1,one_a2,one_a3,one_a4])\n",
     "            a1=torch.mul(a_yuanshi,self.context)\n",
     "            a=torch.tanh(self.liner(a1))\n",
     "            a_softmax=torch.softmax(a,dim=0)\n",
     "            a=torch.sum(a_softmax*a_yuanshi,dim=0)\n",
     "            lista.append(a)\n",
     "        batch_a=torch.stack(lista)\n",
     "        return batch_a  #batch,self.hidden   batch-a表示用户加过注意力后的总和偏好\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "sign = 0\n",
     "class Gru_Recommendation(nn.Module):\n",
     "    def __init__(self,user_sum,item_sum,batch_size,hidden_size,poi_emb_data):\n",
     "        super(Gru_Recommendation, self).__init__()\n",
     "        self.user_sum=user_sum\n",
     "        self.item_sum=item_sum\n",
     "        self.batch_size=batch_size\n",
     "        self.hidden_size=hidden_size\n",
     "\n",
     "        self.user_emb = nn.Embedding(self.user_sum, self.hidden_size)  # 短期和当前使用同一个嵌入向量,\n",
     "        self.item_emb = nn.Embedding(self.item_sum, self.hidden_size)\n",
     "        self.week_emb = nn.Embedding(8, 8)  # 星期\n",
     "        self.check_time_emb = nn.Embedding(25, 25)  # 每天小时\n",
     "\n",
     "\n",
     "\n",
     "        # raise()\n",
     "        #用不到用户的嵌入向量\n",
     "\n",
     "        #初始化嵌入向量\n",
     "        # nn.init.normal_(self.item_emb.weight, std=0.01)\n",
     "        # nn.init.normal_(self.user_emb.weight, std=0.01)\n",
     "        # nn.init.normal_(self.week_emb.weight, std=0.01)\n",
     "        # nn.init.normal_(self.check_time_emb.weight, std=0.01)\n",
     "\n",
     "\n",
     "        self.model=Short_Term_GRU(user_sum,item_sum,batch_size=self.batch_size,hidden_size=self.hidden_size,item_emb=self.item_emb,user_emb=self.user_emb,week_emb=self.week_emb,check_time_emb=self.check_time_emb)\n",
     "        self.current_model=Current_GRU(user_sum,item_sum,batch_size=self.batch_size,hidden_size=self.hidden_size,item_emb=self.item_emb,user_emb=self.user_emb,week_emb=self.week_emb,check_time_emb=self.check_time_emb)\n",
     "        self.attention=Attention(user_sum,item_sum,batch_size=self.batch_size,hidden_size=self.hidden_size)\n",
     "        #self.attention_preference = Attention_perference(hidden_size=hidden_size) #这是源代码\n",
     "        self.attention_preference=Attention_perference(hidden_size=self.hidden_size)#这是我改动了一点，加了个self\n",
     "\n",
     "        ###################考虑的因素个数\n",
     "        # self.fenleiliner = torch.nn.Linear(self.hidden_size*2, self.hidden_size) #【用户向量，关系向量，短期目前交互向量，当前向量】\n",
     "        #单一偏好分类\n",
     "        self.fenleiliner = torch.nn.Linear(self.hidden_size, self.item_sum)  #\n",
     "\n",
     "        self.catliner1 = torch.nn.Linear(self.hidden_size , self.hidden_size)\n",
     "        self.catliner2 = torch.nn.Linear(self.hidden_size*2, self.hidden_size)\n",
     "        self.catliner3 = torch.nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
     "        self.catliner4 = torch.nn.Linear(self.hidden_size*2 +100, self.hidden_size)  #100表示用户关系的嵌入尺寸\n",
     "        ####################################################################\n",
     "\n",
     "        #损失函数\n",
     "        self.crossentropy=nn.CrossEntropyLoss()\n",
     "\n",
     "\n",
     "        self.reset_parameters()  # 初始化变量参数-0.5-0.5之间，理论上正太分布初始化最好\n",
     "\n",
     "\n",
     "        # 对签到时间信息做特殊的处理，使其在训练中不可以被训练\n",
     "        self.week_emb.weight.requires_grad = False\n",
     "        self.check_time_emb.weight.requires_grad=False\n",
     "        self.week_emb.weight.data=torch.zeros(8, 8).scatter_(1, torch.LongTensor([[_] for _ in range(8)]),1)\n",
     "        self.check_time_emb.weight.data=torch.zeros(25, 25).scatter_(1, torch.LongTensor([[_] for _ in range(25)]),1)\n",
     "        # raise()\n",
     "        # self.item_emb.weight.resquires_grad=False\n",
     "        # self.item_emb.weight.data=torch.FloatTensor(poi_emb_data)\n",
     "\n",
     "    def reset_parameters(self):  # 初始化所有定义的参数\n",
     "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
     "        for weight in self.parameters():\n",
     "            weight.data.uniform_(-stdv, stdv)\n",
     "\n",
     "\n",
     "    def forward(self,target_input,month_poi_sequence_input,month_week_sequence_input,month_time_sequence_input,month_sequence_length_input,\n",
     "                                                session_poi_sequence_input,session_week_sequence_input,session_time_sequence_input,session_sequence_length_input,check_history_input,current_batch,user_relation_input,neg_target_input):\n",
     "\n",
     "        #########3                         计算用户的短期偏好\n",
     "        short_perference,lastone_short_perference = self.model.forward(month_poi_sequence_input,month_week_sequence_input,month_time_sequence_input,month_sequence_length_input,current_batch)\n",
     "        lastone_short_perference = lastone_short_perference[0]\n",
     "\n",
     "        #########                          计算用户的当前偏好\n",
     "        current_perference = self.current_model.forward(session_poi_sequence_input,session_week_sequence_input,session_time_sequence_input,session_sequence_length_input,current_batch)\n",
     "        current_perference_real = current_perference  #为什么输出是三维的数据，不修改的原因是不知道current-pre在之后会不会像列表一样会变化呀\n",
     "        ##########################################\n",
     "        #print(\"current_preference:\",len(current_perference))\n",
     "        #print(current_perference)\n",
     "        #############                       计算用户的交互偏好\n",
     "        short_current_interact_perference = self.attention.forward_batch_onebyone(short_perference,current_perference,current_batch,month_sequence_length_input)  #这里面修改了目前的偏好是否会影响外面的偏好\n",
     "        ###################################一定要验证current_perference的值变化了没有。应该没有变化，我都是使用的赋给新值的方法\n",
     "        #print(\"short\",len(short_current_interact_perference),len(short_current_interact_perference[1]))\n",
     "        #print(short_current_interact_perference)\n",
     "\n",
     "        ########                       计算用户的关系偏好\n",
     "        user_relation_perference = Variable(torch.FloatTensor(user_relation_input)).type(ftype)  # 用户关系偏好，偏好预训练得到\n",
     "        ####################################################################################\n",
     "        #print(\"use_relation\",len(user_relation_perference),len(user_relation_perference[1]))\n",
     "        #print(user_relation_perference)\n",
     "        ###                   计算用户的偏好偏好的整合方式@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
     "\n",
     "        # sum-perference表示偏好的混合\n",
     "        ############################使用单一的偏好模型\n",
     "        # sum_perference=lastone_short_perference  #使用短期偏好   #准确率可以达到100%\n",
     "        # sum_perference=current_perference_real[0]  #使用目前偏好   #准确率先升40再降36最后可以达到100%\n",
     "        # sum_perference=short_current_interact_perference #使用交互偏好  #准确率可以达到100%，但是拟合速度有点慢\n",
     "        # sum_perference=user_history_perference\n",
     "        # sum_perference =user_relation_perference\n",
     "        # sum_perference = self.catliner1(sum_perference)  # 后面有加tanh\n",
     "        # 说明每种偏好都能够成功的对用户的签到序列进行建模\n",
     "        ###################################################\n",
     "\n",
     "\n",
     "        ############全加   偏好拼接  训练集能达到100%说明当前和短期偏好都有影响\n",
     "        ############当前和交互过的偏好。大约10次能够预测训练集中的90%左右了。\n",
     "        sum_perference=torch.cat(\n",
     "            [\n",
     "                user_relation_perference,   #用户关系偏好\n",
     "                short_current_interact_perference,  #周期偏好\n",
     "                current_perference_real[0],         #当前偏好\n",
     "            ], dim=-1)   #历史访问兴趣点偏好\n",
     "        sum_perference = self.catliner4(sum_perference)  # 后面有加tanh\n",
     "        # sum_perference = self.catliner4(sum_perference)  # 后面有加tanh\n",
     "        # sum_perference = self.catliner4(sum_perference)  # 后面有加tanh\n",
     "        # sum_perference = self.catliner4(sum_perference)  # 后面有加tanh\n",
     "        ##################################3\n",
     "\n",
     "\n",
     "        # #方法2 使用偏好相加，平均或者相乘的形式表示用户的混合偏好\n",
     "        # #方法2 使用偏好相加，平均或者相乘的形式表示用户的混合偏好\n",
     "        # ##################################################\n",
     "        # #相加                     训练数据集上拟合可以达到100%   current-preference—real和current-preference相等\n",
     "        # # sum_perference=torch.add(current_perference_real[0],last_short_perference[0])\n",
     "        #                         # 3个相加也能达到100%准确率\n",
     "        # # sum_perference=torch.add(torch.add(current_perference[0],last_short_perference),short_current_interact_perference)\n",
     "        #\n",
     "        # sum_perference=torch.add(current_perference[0],short_current_interact_perference)\n",
     "        # sum_perference=torch.add(sum_perference,user_relation_perference)  #用户关系\n",
     "        # sum_perference=torch.add(sum_perference,user_history_perference)  #用户历史偏好\n",
     "        # #\n",
     "        # #\n",
     "        # # #平均                     平均也能达到100%准确率\n",
     "        # sum_perference = sum_perference\n",
     "        #相乘      还没实验   方法不好，很难拟合，而且迭代慢\n",
     "        #sum_perference = torch.mul(current_perference_real[0], lastone_short_perference)  #迭代很慢\n",
     "\n",
     "\n",
     "\n",
     "        ###方法3：使用注意力整合用户的综合偏好\n",
     "        # sum_perference=self.attention_preference(\n",
     "        #     user_relation_perference,\n",
     "        #     short_current_interact_perference,\n",
     "        #     current_perference_real[0],\n",
     "        #     user_history_perference\n",
     "        # )\n",
     "        ####\n",
     "\n",
     "        sum_perference=torch.tanh(sum_perference)    #好像训练速度（参数不变的情况下，迭代次数）会有点影响，准确率没有任何影响\n",
     "        # sum_perference=torch.relu(sum_perference)  #还没有测试两者的区别？？？？\n",
     "        ###########################################################################################3\n",
     "        #print(\"sum_preference\",len(sum_perference),len(sum_perference[1]))\n",
     "        #print(sum_perference)\n",
     "\n",
     "        ################                 交叉熵损失计算（将其看成分类任务）分为使用内积和全连接两种   #############3\n",
     "        #####最终偏好的计算finall—perference的计算\n",
     "        ##########方法1               使用单一偏好能够训练100%数据。 （最优模型）\n",
     "        ############ 内积形式使用偏好和嵌入兴趣点的表示用户访问兴趣点的偏好,这种方法没毛病，可能不是最好的。但一定不差\n",
     "        # finally_perference_score=torch.matmul(sum_perference,Variable(torch.t(self.item_emb.weight)))  #大约迭代15次达到100%\n",
     "        finally_perference_score = torch.matmul(sum_perference, torch.t(self.item_emb.weight))  # 大约迭\n",
     "        #print(\"zheshifenshu ---------------------------\",finally_perference_score,\"kankanxiaoguoba--------------------------\")\n",
     "        #########损失函数：交叉熵,bpr,负采样  bpr和负采样作为损失函数本文都还没有使用\n",
     "        ##########计算最终的n个兴趣点分值偏好于目标兴趣点的误差，使用交叉熵函数\n",
     "        loss=self.crossentropy(finally_perference_score, Variable(torch.LongTensor(target_input).type(ltype)))\n",
     "        return finally_perference_score.cpu().detach().numpy(), loss\n",
     "#事先从当前序列的K近邻序列中计算出每个兴趣点在该序列可能出现的概率，存到一个本地文件\n",
     "df = pd.read_table('testSco100.txt',sep=' ', header = None)\n",
     "df = df.values\n",
     "ftarget=open(\"target.txt\",\"r\")  #读取用户ID和目标\n",
     "targetlist=[]\n",
     "id_list=[]\n",
     "for i in ftarget:\n",
     "    a=int(i.split(\"\\t\")[-1].replace(\"\\n\",\"\"))\n",
     "    id_list.append(int(i.split(\"\\t\")[0]))\n",
     "    targetlist.append(a)\n",
     "\n",
     "#1\t4,6,4,5,4,3,2\t0,1,5,6,6,3,3\t1,19,3,15,19,19,22\n",
     "fmonth=open(\"month.txt\",\"r\")  #读取序列的最大长度\n",
     "month_max_length=0\n",
     "for i in fmonth:\n",
     "    a=len(i.split(\"\\t\")[1].split(\",\"))\n",
     "    if(a>month_max_length):\n",
     "        month_max_length=a\n",
     "fmonth.close()\n",
     "fmonth=open(\"month.txt\",\"r\")\n",
     "month_poi_sequence=[]\n",
     "month_week_sequence=[]\n",
     "month_time_sequence=[]\n",
     "id_list=[]\n",
     "month_sequence_length=[]\n",
     "for i in fmonth:\n",
     "    a=i.replace(\"\\n\",\"\").split(\"\\t\")\n",
     "    id_list.append(a[0])\n",
     "    month_sequence_length.append(len(a[1].split(\",\")))\n",
     "    if(len(a[1].split(\",\"))<=month_max_length):\n",
     "        one_poi_sequence=a[1].split(\",\")\n",
     "        one_poi_sequence.extend([0 for i in range(month_max_length-len(one_poi_sequence))])\n",
     "        one_week_sequence=a[2].split(\",\")\n",
     "        one_week_sequence.extend([0 for i in range(month_max_length-len(one_week_sequence))])  #这里面0就有含义了，应该不能用0，先这样\n",
     "        one_time_sequence=a[3].split(\",\")\n",
     "        one_time_sequence.extend([0 for i in range(month_max_length-len(one_time_sequence))])\n",
     "    month_poi_sequence.append(one_poi_sequence)\n",
     "    month_week_sequence.append(one_week_sequence)\n",
     "    month_time_sequence.append(one_time_sequence)\n",
     "fmonth.close()\n",
     "\n",
     "fsession=open(\"session.txt\",\"r\")  #读取序列的最大长度\n",
     "session_max_length=0\n",
     "for i in fsession:\n",
     "    a=len(i.split(\"\\t\")[1].split(\",\"))\n",
     "    if(a>session_max_length):\n",
     "        session_max_length=a\n",
     "fsession.close()\n",
     "\n",
     "\n",
     "fsession=open(\"session.txt\",\"r\")\n",
     "session_poi_sequence=[]\n",
     "session_week_sequence=[]\n",
     "session_time_sequence=[]\n",
     "session_sequence_length=[]\n",
     "for i in fsession:\n",
     "    a=i.replace(\"\\n\",\"\").split(\"\\t\")\n",
     "    session_sequence_length.append(len(a[1].split(\",\")))\n",
     "    if(len(a[1].split(\",\"))<session_max_length):\n",
     "        one_poi_sequence=a[1].split(\",\")\n",
     "        one_poi_sequence.extend([0 for i in range(session_max_length-len(one_poi_sequence))])\n",
     "        one_week_sequence=a[2].split(\",\")\n",
     "        one_week_sequence.extend([0 for i in range(session_max_length-len(one_week_sequence))])  #这里面0就有含义了，应该不能用0，先这样\n",
     "        one_time_sequence=a[3].split(\",\")\n",
     "        one_time_sequence.extend([0 for i in range(session_max_length-len(one_time_sequence))])\n",
     "    session_poi_sequence.append(one_poi_sequence)\n",
     "    session_week_sequence.append(one_week_sequence)\n",
     "    session_time_sequence.append(one_time_sequence)\n",
     "fsession.close()\n",
     "\n",
     "users_checkins_dict={}\n",
     "fuser_check=open(\"user_check.txt\",\"r\",encoding=\"utf-8\")\n",
     "for i in fuser_check:\n",
     "    a=i.strip().split(\"\\t\")\n",
     "    users_checkins_dict[int(a[0])]=[int(_) for _ in a[1].split(\",\")]\n",
     "    if(len(users_checkins_dict[int(a[0])])>=100):\n",
     "        users_checkins_dict[int(a[0])]=users_checkins_dict[int(a[0])][:100]  #只取用户最近签到的100个数据作为历史数据\n",
     "        pass\n",
     "    else:\n",
     "        users_checkins_dict[int(a[0])].extend([0 for i in range(100-len(users_checkins_dict[int(a[0])]))])\n",
     "        # pass\n",
     "fuser_check.close()\n",
     "\n",
     "# raise()\n",
     "\n",
     "\n",
     "# ####################分割测试集和训练集\n",
     "t_id_index_list=[]  #读取列表第一个作为测试集，因为签到是逆序的：：：注意\n",
     "t_id_dict={}\n",
     "for i in range(len(id_list)):\n",
     "    try:\n",
     "        t_id_dict[int(id_list[i])]\n",
     "    except:\n",
     "        t_id_dict[int(id_list[i])]=1\n",
     "        t_id_index_list.append(int(i))\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "# if 1:\n",
     "t_target=[]\n",
     "t_month_poi_sequence=[]\n",
     "t_month_week_sequence=[]\n",
     "t_month_time_sequence=[]\n",
     "t_month_sequence_length=[]\n",
     "t_session_poi_sequence=[]\n",
     "t_session_week_sequence=[]\n",
     "t_session_time_sequence=[]\n",
     "t_session_sequence_length=[]\n",
     "t_id=[]\n",
     "\n",
     "\n",
     "#删除索引从后向前，防止前面索引变化\n",
     "t_id_index_list=t_id_index_list[::-1]   #逆序，先删除索引大的.里面的值表示索引\n",
     "for i in t_id_index_list:\n",
     "    try:\n",
     "        t_month_poi_sequence.append(month_poi_sequence[i])\n",
     "        del month_poi_sequence[i]\n",
     "    except:\n",
     "        print(\"a\")\n",
     "        pass\n",
     "\n",
     "    t_month_week_sequence.append(month_week_sequence[i])\n",
     "    del month_week_sequence[i]\n",
     "\n",
     "    t_month_time_sequence.append(month_time_sequence[i])\n",
     "    del month_time_sequence[i]\n",
     "\n",
     "    t_month_sequence_length.append(month_sequence_length[i])\n",
     "    del month_sequence_length[i]\n",
     "\n",
     "    t_session_poi_sequence.append(session_poi_sequence[i])\n",
     "    del session_poi_sequence[i]\n",
     "\n",
     "    t_session_week_sequence.append(session_week_sequence[i])\n",
     "    del session_week_sequence[i]\n",
     "\n",
     "    t_session_time_sequence.append(session_time_sequence[i])\n",
     "    del session_time_sequence[i]\n",
     "\n",
     "    t_session_sequence_length.append(session_sequence_length[i])\n",
     "    del session_sequence_length[i]\n",
     "\n",
     "    t_target.append(targetlist[i])\n",
     "    del targetlist[i]\n",
     "\n",
     "    t_id.append(id_list[i])\n",
     "    del id_list[i]\n",
     "# ########################分割训练和测试\n",
     "\n",
     "\n",
     "###############################初始化模型参数\n",
     "# raise()\n",
     "# batch_size=3\n",
     "\n",
     "# raise()\n",
     "batch_size=16\n",
     "hidden_size=240\n",
     "# user_sum=2195\n",
     "# item_sum=3521\n",
     "user_sum=2195\n",
     "item_sum=3521\n",
     "\n",
     "epoch=20\n",
     "shujuliang=1000#以上数据都是基于数据集为500，batch-size为3测试得到的\n",
     "# shujuliang=10000\n",
     "learn_rate=0.01   #学习率被我固定了\n",
     "momentum=0.9\n",
     "weight_decay=0.0001\n",
     "################################################################################\n",
     "\n",
     "print(\"user_sum:\"+str(user_sum))\n",
     "print(\"item_sum:\"+str(item_sum))\n",
     "print(\"batch_size:\"+str(batch_size))\n",
     "print(\"hidden_size:\"+str(hidden_size))\n",
     "print(\"eopch:\"+str(epoch))\n",
     "print(\"learn_rate:\"+str(learn_rate))\n",
     "print(\"momentum:\"+str(momentum))\n",
     "print(\"weight_decay:\"+str(weight_decay))\n",
     "\n",
     "\n",
     "\n",
     "poi_relation=[]\n",
     "#用户的嵌入数据写入\n",
     "# print(\"写入POI关系文件,txt\")\n",
     "# from gensim.models import KeyedVectors\n",
     "# wv = KeyedVectors.load(\"数据集处理/POI_sum_relation_500.kv\", mmap='r')\n",
     "# # vector = wv['1']  # numpy vector of a word\n",
     "# poi_relation=np.zeros((item_sum,hidden_size))  #用户总数已经是多1了的\n",
     "# for i in range(1,item_sum):\n",
     "#     # print(i)\n",
     "#     try:\n",
     "#         poi_relation[i]=wv[str(i)]   #str必须是字典\n",
     "#     except:\n",
     "#         poi_relation[i]=[0 for i in range(100)]\n",
     "#         print(\"warning:目标 \"+str(i)+\" 不再嵌入表中！\")\n",
     "\n",
     "\n",
     "#声明模型\n",
     "if(use_cuda==1):\n",
     "    gru_recommendation=Gru_Recommendation(user_sum,item_sum,batch_size=batch_size,hidden_size=hidden_size,poi_emb_data=poi_relation).cuda()\n",
     "else:\n",
     "    gru_recommendation = Gru_Recommendation(user_sum, item_sum, batch_size=batch_size, hidden_size=hidden_size,poi_emb_data=poi_relation)\n",
     "\n",
     "#优化器  SGD优化器，Adam优化器，其他优化器\n",
     "optimizer=optim.SGD(gru_recommendation.parameters(),lr=learn_rate,momentum=momentum\n",
     "                    ,weight_decay=weight_decay\n",
     "                    )\n",
     "\n",
     "######################训练数据排序\n",
     "month_sequence_length=np.asarray(month_sequence_length,dtype=int)\n",
     "# print(month_sequence_length)\n",
     "arg_sort_list = month_sequence_length.argsort()[::-1]  #找到从小到大的排序索引\n",
     "\n",
     "#找到从下到大的排序坐标\n",
     "targetlist=np.asarray(targetlist,dtype=int)[arg_sort_list]\n",
     "\n",
     "\n",
     "#保持所有类别的序列对应关系\n",
     "id_list=np.asarray(id_list,dtype=int)[arg_sort_list]\n",
     "\n",
     "\n",
     "month_sequence_length=np.asarray(month_sequence_length,dtype=int)[arg_sort_list]\n",
     "month_poi_sequence=np.asarray(month_poi_sequence,dtype=int)[arg_sort_list]\n",
     "month_week_sequence=np.asarray(month_week_sequence,dtype=int)[arg_sort_list]\n",
     "month_time_sequence=np.asarray(month_time_sequence,dtype=int)[arg_sort_list]\n",
     "\n",
     "session_sequence_length=np.asarray(session_sequence_length,dtype=int)[arg_sort_list]\n",
     "session_poi_sequence=np.asarray(session_poi_sequence,dtype=int)[arg_sort_list]\n",
     "session_week_sequence=np.asarray(session_week_sequence,dtype=int)[arg_sort_list]\n",
     "session_time_sequence=np.asarray(session_time_sequence,dtype=int)[arg_sort_list]\n",
     "########################################################################################\n",
     "\n",
     "\n",
     "\n",
     "#################             测试数据排序             ##################################\n",
     "t_month_sequence_length=np.asarray(t_month_sequence_length,dtype=int)\n",
     "# print(month_sequence_length)\n",
     "t_arg_sort_list = t_month_sequence_length.argsort()[::-1]  #找到从小到大的排序索引\n",
     "\n",
     "#找到从下到大的排序坐标\n",
     "t_targetlist=np.asarray(t_target,dtype=int)[t_arg_sort_list]   #找到排序后的大小值，是t_targetlist,不是t_target\n",
     "\n",
     "#保持所有类别的序列对应关系\n",
     "t_id=np.asarray(t_id,dtype=int)[t_arg_sort_list]\n",
     "\n",
     "t_month_sequence_length=np.asarray(t_month_sequence_length,dtype=int)[t_arg_sort_list]\n",
     "t_month_poi_sequence=np.asarray(t_month_poi_sequence,dtype=int)[t_arg_sort_list]\n",
     "t_month_week_sequence=np.asarray(t_month_week_sequence,dtype=int)[t_arg_sort_list]\n",
     "t_month_time_sequence=np.asarray(t_month_time_sequence,dtype=int)[t_arg_sort_list]\n",
     "\n",
     "t_session_sequence_length=np.asarray(t_session_sequence_length,dtype=int)[t_arg_sort_list]\n",
     "t_session_poi_sequence=np.asarray(t_session_poi_sequence,dtype=int)[t_arg_sort_list]\n",
     "t_session_week_sequence=np.asarray(t_session_week_sequence,dtype=int)[t_arg_sort_list]\n",
     "t_session_time_sequence=np.asarray(t_session_time_sequence,dtype=int)[t_arg_sort_list]\n",
     "###################################################################################################\n",
     "\n",
     "\n",
     "iter_sum=0\n",
     "#迭代次数重新计算 训练集\n",
     "if(len(session_poi_sequence)%batch_size==0):\n",
     "    iter_sum=int(len(session_poi_sequence)/batch_size)\n",
     "else:\n",
     "    print(str(len(session_poi_sequence)%batch_size)+\",训练集不为0,最后有剩余\")\n",
     "    # raise()\n",
     "    iter_sum=int(len(session_poi_sequence)/batch_size)+1\n",
     "shujuliang_sum=iter_sum\n",
     "\n",
     "\n",
     "#测试集合迭代次数重新计算\n",
     "t_iter_sum=0\n",
     "if(len(t_session_poi_sequence)%batch_size==0):\n",
     "    t_iter_sum=int(len(t_session_poi_sequence)/batch_size)\n",
     "\n",
     "else:\n",
     "    t_iter_sum=int(len(t_session_poi_sequence)/batch_size)+1\n",
     "    print(str(len(t_session_poi_sequence) % batch_size) + \",测试集不为0，最后迭代有剩余\")\n",
     "    # raise()\n",
     "t_shujuliang=t_iter_sum\n",
     "\n",
     "\n",
     "\n",
     "print(\"写入用户关系文件,txt\")    #用户关系文件被嵌入到100维度了\n",
     "from gensim.models import KeyedVectors\n",
     "wv = KeyedVectors.load(\"sum_relation_500.kv\", mmap='r')\n",
     "# vector = wv['1']  # numpy vector of a word\n",
     "user_relation_dict={}\n",
     "for i in range(1,user_sum):\n",
     "    # print(i)\n",
     "    try:\n",
     "        user_relation_dict[i]=wv[str(i)]   #str必须是字典\n",
     "    except:\n",
     "        user_relation_dict[i]=[0 for i in range(100)]\n",
     "        #print(\"warning:目标 \"+str(i)+\" 不再嵌入表中！\")\n",
     "# raise()\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "# print(\"随机生成用户关系序列.txt\")\n",
     "# user_relation_dict=np.random.randn(3000,100)\n",
     "# print(user_relation_dict.shape)\n",
     "# print(user_relation_dict)\n",
     "# raise()\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "best={\"hit_sum_top1\":0,\"hit_sum_top5\":0,\"hit_sum_top10\":0,\n",
     "      \"ceshi_index_top1\":0,\"ceshi_index_top5\":0,\"ceshi_index_top10\":0}\n",
     "\n",
     "\n",
     "def findnegtarget(neg_id):\n",
     "    neg_item=random.randint(1,item_sum)\n",
     "    while(neg_item in users_checkins_dict[neg_id]):\n",
     "        neg_item = random.randint(1, item_sum)\n",
     "\n",
     "    return neg_item\n",
     "\n",
     "def start_predict(batch_size=1):\n",
     "    ###########                          开始预测\n",
     "    print(\"开始预测\")\n",
     "\n",
     "\n",
     "    hit_sum_top1 = 0\n",
     "    hit_sum_top5 = 0\n",
     "    hit_sum_top10 = 0\n",
     "    ceshi_index_top1 = 0\n",
     "    ceshi_index_top5 = 0\n",
     "    ceshi_index_top10 = 0\n",
     "    ceshisum = 0\n",
     "    ceshilosssum = 0\n",
     "    t_shujuliang=len(t_target)\n",
     "    #print(\"数据量：\"+str(t_shujuliang))\n",
     "\n",
     "    for j in range(t_shujuliang):\n",
     "        if (j == t_shujuliang - 1):\n",
     "            print(\"h\")\n",
     "        t_target_input = t_targetlist[j * batch_size:(j + 1) * batch_size]\n",
     "        t_month_poi_sequence_input = np.asarray(t_month_poi_sequence[j * batch_size:(j + 1) * batch_size], dtype=int)\n",
     "        t_month_week_sequence_input = np.asarray(t_month_week_sequence[j * batch_size:(j + 1) * batch_size], dtype=int)\n",
     "        t_month_time_sequence_input = np.asarray(t_month_time_sequence[j * batch_size:(j + 1) * batch_size], dtype=int)\n",
     "        t_month_sequence_length_input = np.asarray(t_month_sequence_length[j * batch_size:(j + 1) * batch_size],\n",
     "                                                   dtype=int)\n",
     "        t_session_poi_sequence_input = np.asarray(t_session_poi_sequence[j * batch_size:(j + 1) * batch_size],\n",
     "                                                  dtype=int)\n",
     "        t_session_week_sequence_input = np.asarray(t_session_week_sequence[j * batch_size:(j + 1) * batch_size],\n",
     "                                                   dtype=int)\n",
     "        t_session_time_sequence_input = np.asarray(t_session_time_sequence[j * batch_size:(j + 1) * batch_size],\n",
     "                                                   dtype=int)\n",
     "        t_session_sequence_length_input_real = np.asarray(\n",
     "            t_session_sequence_length[j * batch_size:(j + 1) * batch_size], dtype=int)\n",
     "\n",
     "        t_id_input = np.asarray(t_id[j * batch_size:(j + 1) * batch_size], dtype=int)\n",
     "        check_history_input = np.asarray([users_checkins_dict[int(_)] for _ in t_id_input], dtype=int)\n",
     "\n",
     "        user_relation_input = np.asarray([user_relation_dict[int(_)] for _ in t_id_input], dtype=float)\n",
     "\n",
     "        # session_sequence_length_input = [15 for i in range(batch_size)]\n",
     "        current_batch = len(t_target_input)  # 防止最后一个batch_size不是2，是1\n",
     "        # 目标函数计算损失不需要负采样，也和预测无关，因此我随便选择了一个函数\n",
     "        predict_logit, ceshiloss = gru_recommendation(t_target_input, t_month_poi_sequence_input,\n",
     "                                                      t_month_week_sequence_input,\n",
     "                                                      t_month_time_sequence_input, t_month_sequence_length_input,\n",
     "                                                      t_session_poi_sequence_input, t_session_week_sequence_input,\n",
     "                                                      t_session_time_sequence_input, t_session_sequence_length_input_real,\n",
     "                                                      check_history_input, current_batch, user_relation_input,\n",
     "                                                      t_target_input)\n",
     "        # print(\"ce\")\n",
     "        # print(ceshiloss)\n",
     "        # time.sleep(1)\n",
     "        #print(predict_logit)\n",
     "        for i in range(len(predict_logit[0])):\n",
     "            predict_logit[0][i]=predict_logit[0][i]+df[i][int(t_id_input)]\n",
     "\n",
     "        ceshilosssum += ceshiloss\n",
     "        for one_predict, one_target, one_id in zip(predict_logit, t_target_input, t_id_input):\n",
     "            #print(len(one_predict),\"one_preict:\",one_predict)\n",
     "            ceshisum += 1\n",
     "            topk = list(np.asarray(one_predict).argsort()[-10:])[::-1]  # 排序索引\n",
     "            # print(\"this is topk :\",topk)\n",
     "            if (int(one_target) in topk):\n",
     "                hit_sum_top10 += 1\n",
     "                one_index = topk.index(int(one_target)) + 1  # 索引从1开始计数\n",
     "                ceshi_index_top10 += 1.0 / one_index\n",
     "\n",
     "                if (int(one_target) in topk[:5]):\n",
     "                    hit_sum_top5 += 1\n",
     "                    one_index = topk[:5].index(int(one_target)) + 1\n",
     "                    ceshi_index_top5 += 1.0 / one_index\n",
     "\n",
     "                    if (int(one_target) in topk[:1]):\n",
     "                        hit_sum_top1 += 1\n",
     "                        one_index = topk[:1].index(int(one_target)) + 1\n",
     "                        ceshi_index_top1 += 1.0 / one_index\n",
     "                # print(\"id:\"+str(one_id))\n",
     "                # print(topk)\n",
     "                # print(\"__________________\")\n",
     "                # print(one_target)\n",
     "\n",
     "                # topk.index(one_target)\n",
     "                # raise()\n",
     "        # summ+=1\n",
     "        # if(summ>shujuliang):\n",
     "        #     break\n",
     "\n",
     "    hit_sum_top1 = hit_sum_top1 / ceshisum\n",
     "    hit_sum_top5 = hit_sum_top5 / ceshisum\n",
     "    hit_sum_top10 = hit_sum_top10 / ceshisum\n",
     "    if (hit_sum_top1 > best[\"hit_sum_top1\"]):\n",
     "        best[\"hit_sum_top1\"] = hit_sum_top1\n",
     "    if (hit_sum_top5 > best[\"hit_sum_top5\"]):\n",
     "        best[\"hit_sum_top5\"] = hit_sum_top5\n",
     "    if (hit_sum_top10 > best[\"hit_sum_top10\"]):\n",
     "        best[\"hit_sum_top10\"] = hit_sum_top10\n",
     "\n",
     "    ceshi_index_top1 = ceshi_index_top1 / ceshisum\n",
     "    ceshi_index_top5 = ceshi_index_top5 / ceshisum\n",
     "    ceshi_index_top10 = ceshi_index_top10 / ceshisum\n",
     "    if (ceshi_index_top1 > best[\"ceshi_index_top1\"]):\n",
     "        best[\"ceshi_index_top1\"] = ceshi_index_top1\n",
     "    if (ceshi_index_top5 > best[\"ceshi_index_top5\"]):\n",
     "        best[\"ceshi_index_top5\"] = ceshi_index_top5\n",
     "    if (ceshi_index_top10 > best[\"ceshi_index_top10\"]):\n",
     "        best[\"ceshi_index_top10\"] = ceshi_index_top10\n",
     "\n",
     "    print(\"Experiment result: \",best)\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "\n",
     "########################         主函数            #####################\n",
     "if 1:\n",
     "    for i in range(epoch):\n",
     "        print(\"epoch   \"+str(i)+\"\\n\")\n",
     "        ####################################################### 训练\n",
     "        epoch_loss=0\n",
     "        summ=0\n",
     "        shujuliang=[_ for _ in range(shujuliang_sum)]\n",
     "        random.shuffle(shujuliang)\n",
     "        #print(\"this is shujuliang\",shujuliang[:10])\n",
     "        for j in tqdm.tqdm(shujuliang):\n",
     "            target_input=targetlist[j*batch_size:(j+1)*batch_size]\n",
     "            month_poi_sequence_input=np.asarray(month_poi_sequence[j*batch_size:(j+1)*batch_size],dtype=int)\n",
     "            month_week_sequence_input=np.asarray(month_week_sequence[j*batch_size:(j+1)*batch_size],dtype=int)\n",
     "            month_time_sequence_input=np.asarray(month_time_sequence[j*batch_size:(j+1)*batch_size],dtype=int)\n",
     "            month_sequence_length_input=np.asarray(month_sequence_length[j*batch_size:(j+1)*batch_size],dtype=int)\n",
     "            session_poi_sequence_input=np.asarray(session_poi_sequence[j*batch_size:(j+1)*batch_size],dtype=int)\n",
     "            session_week_sequence_input=np.asarray(session_week_sequence[j*batch_size:(j+1)*batch_size],dtype=int)\n",
     "            session_time_sequence_input=np.asarray(session_time_sequence[j*batch_size:(j+1)*batch_size],dtype=int)\n",
     "            session_sequence_length_input_real=np.asarray(session_sequence_length[j*batch_size:(j+1)*batch_size],dtype=int)\n",
     "            id_input=np.asarray(id_list[j*batch_size:(j+1)*batch_size],dtype=int)\n",
     "            # neg_target_input=np.asarray([findnegtarget(_) for _ in id_input],dtype=int)\n",
     "            neg_target_input =[0]\n",
     "\n",
     "            check_history_input=np.asarray([users_checkins_dict[int(_)] for _ in id_input],dtype=int)\n",
     "\n",
     "            user_relation_input = np.asarray([user_relation_dict[int(_)] for _ in id_input], dtype=float)\n",
     "            # month_sequence_length_input=[165,165]\n",
     "            # session_sequence_length_input=[session_max_length for _ in range(batch_size)]\n",
     "            current_batch=len(target_input)  #防止最后一个batch_size不是2，是1\n",
     "\n",
     "\n",
     "            optimizer.zero_grad()\n",
     "            _,loss=gru_recommendation(target_input,month_poi_sequence_input,month_week_sequence_input,month_time_sequence_input,month_sequence_length_input,\n",
     "                                                session_poi_sequence_input,session_week_sequence_input,session_time_sequence_input,session_sequence_length_input_real,check_history_input,current_batch,user_relation_input,neg_target_input)\n",
     "            # print(loss)\n",
     "            epoch_loss+=loss\n",
     "            loss.backward()\n",
     "            #防止梯度爆炸\n",
     "            torch.nn.utils.clip_grad_value_(gru_recommendation.parameters(), 2)\n",
     "            optimizer.step()\n",
     "            # time.sleep(2)\n",
     "            # print(epoch_loss)\n",
     "            # summ+=1\n",
     "            # if(summ>shujuliang):\n",
     "            #     break\n",
     "            # print(gru_recommendation.week_emb.weight[1])\n",
     "            # time.sleep(3)\n",
     "            if(j%4000==0):\n",
     "                start_predict()\n",
     "\n",
     "        print(epoch_loss)\n",
     "        print(\"一次训练完成\")\n",
     "        ################################################################################################\n",
     "\n",
     "\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}